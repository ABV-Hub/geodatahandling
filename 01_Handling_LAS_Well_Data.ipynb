{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-Handling LAS Well Data.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Pij2PSOOXJ1x",
        "SqnzgER3GQdC",
        "cFhLNSyaX1qZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuEfxdD5vLuq",
        "colab_type": "text"
      },
      "source": [
        "<table align=\"center\">\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/drive/1lwfD4diA6-Pn6yiWfzUtt9BqQCVfK7NO\">\n",
        "        <img src=\"https://i.ibb.co/3723Hm9/colab.png\"  style=\"padding-bottom:5px;\" />Run in Google Colab</a></td>\n",
        "  <td align=\"center\"><a target=\"_blank\" href=\"https://github.com/leocd91/geodatahandling\">\n",
        "        <img src=\"https://i.ibb.co/L5p10GH/github.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  />View Source on GitHub</a></td>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pij2PSOOXJ1x",
        "colab_type": "text"
      },
      "source": [
        "# **Introduction** (you can read it or skip it)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITe_TZChEczk",
        "colab_type": "text"
      },
      "source": [
        "By : leocd91@gmail.com\n",
        "\n",
        "---\n",
        "Welcome to the first episode of \"***Digital Geoscience Data Handling using Python***\" Series.\n",
        "During this physical-distancing-new-normal year, I try to compile my experiences in handling digital geoscience / petrotechnical data. \n",
        "\n",
        "This will include: Parsing any type of data, QC analysis, database, feeding data for geophysical inversion - machine learning / deep learning stuffs, and maybe handling those data to do some GP-GPU too.\n",
        "\n",
        "I made this tutorial as beginner-friendly as possible, so some explanation might not be as \"*cool & marketing-friendly*\" as developers intended. \n",
        "And some code are not *pythonic* for that reason too.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "any feedback just hmu.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jwg3LODIctIj",
        "colab_type": "text"
      },
      "source": [
        "**- Why don't we just install X to read those files?**\n",
        "\n",
        "> Yes you can do that too. \n",
        "\n",
        "> This tutorial for those who wish how to parse any digital geoscience data content without dependencies. \n",
        "> People who work with geoscience data for some years often struggles with python package that evolve really fast which make their code obsolete quickly. Also some package use their own class/data-type which you will find challenging to inspect your own array."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZguU1ugJUAd",
        "colab_type": "text"
      },
      "source": [
        "**- Is there any Prereq.?**\n",
        "\n",
        "> Just knowing basic stuff about programming is enough.\n",
        "If you don't even know what an array is, I suggest watching some crash course python on youtube."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPdihNIKLSie",
        "colab_type": "text"
      },
      "source": [
        " **- Who are you giving some tutorials on the internet!?**\n",
        "\n",
        " \n",
        "\n",
        "> I'm a Petrotechnical Data Management on a NOC in Indonesia. More than 5 years here and still going strong.\n",
        "\n",
        "> I code my way to finish my undergraduate thesis (FDTD Elastic Wave in cuda) and get some side-gigs from there too (Inversion Method, Numerical Simulation, Etc.).\n",
        "\n",
        "> I know some C, F95, and Matlab (Now I'm Python *Muallaf* tho).\n",
        "\n",
        "> My main interest is in computational geophysics and GP-GPU.\n",
        "\n",
        "> I also *(lazily)* wrote some stuff on my blog about computational geophysics here http://redigitize.blogspot.com/ \n",
        "\n",
        "> I also ***love*** deep-fried banana.\n",
        " \n",
        "\n",
        " \n",
        " **TL;DR** I'm not a PhD, CEO, or Someone famous. I'm just glad that I can share something that I learn and learn more from this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKxxEXbmRZIb",
        "colab_type": "text"
      },
      "source": [
        "**- Which Python or Software or IDE or whatever that means to learn python? What is this stuff called Notebook!? Even this google colab thing!?   I hate you!**\n",
        "\n",
        "\n",
        "> Grab your towel and don't panic! \n",
        "> You can use this stuff inside google colab thing which people usually call by \"notebook format\", it's easy to make a step-by-step tutorial here. Also to run your specific line of code you just have to click the \"play\" button (it's actually RUN button tho) on the left.\n",
        "\n",
        "> Try this one below!\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jhWkegLUSjJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"hello stranger, what are you buyin'?\") #click run button on the left and see result below."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FToGcsCUrPR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "> As for what \"*software*\" to use, For me, I rarely use a notebook, some people that prefer IDE or Editor type of python use pycharm (https://www.jetbrains.com/pycharm/) or SPYDER (https://www.spyder-ide.org/) (for former Matlab user, maybe spyder is your favorite editor. It got variable explorer!)\n",
        "\n",
        "**TL;DR** you can use google colab notebook or install pycharm or install anaconda (got spyder and stuff installed in your pc already like google colab)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqnzgER3GQdC",
        "colab_type": "text"
      },
      "source": [
        "# **LAS DATA HANDLING**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GdeORNGhmGpy",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Now how do we parse or load LAS data using Python?\n",
        "First, just like every people using python, We put this 3 lines of code on top of our code. We have this habit to always use this just like some cult chants to summon a Dark Lord.\n",
        "```\n",
        "import numpy as np\n",
        "import panda as pd\n",
        "import matplotlib.pyplot as plt\n",
        "```\n",
        "\n",
        "**What are those!?**\n",
        "\n",
        "> Those stuff are package, function, or module with specific feats that you will use in your code. Let's say Python is a clean install Operating System (OS) on your PC / Laptop but still you need to install some other software to do something that your default OS software can't do. Taken from each documentations:\n",
        "\n",
        "> **Numpy** is a fundamental package for scientific computing with Python. It contains among other things: a powerful N-dimensional array object.\n",
        "\n",
        "> **Pandas** is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool.\n",
        "\n",
        "> **Matplotlib** is a comprehensive library for creating static, animated, and interactive visualizations in Python. \n",
        "\n",
        "Well, If that's too much, let's just load numpy then for a moment.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysZU60I3SnTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "#fundamental package for scientific computing, lets just say it's what makes python can do matlab stuff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65BoWd3GAIWf",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Uhh.. nothing happened? That's because we only imported it. I'll tell you what it does later in the code.\n",
        "\n",
        "Ok then, now for the LAS data that we will use, you can either **load** your data here using that **folder** icon on the left OR **download** open data from the internet.\n",
        "\n",
        "For the last option, you can just run the code bellow.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ke5lBCpHdCzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://www.minnelusa.com/sample_data/4771-36-SESE.las"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9OQh0dwtDnM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Nice. Now let us read those file by running these code below.\n",
        "\n",
        "Those descriptions with green text after the comment (#) are describing what each line does. \n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xPv0Xek_d2PF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename=\"4771-36-SESE.las\"       #put file's name to \"filename\" variable\n",
        "myfile = open(filename)           #open LAS file\n",
        "data = myfile.read()              #read myfile content, then put it to \"data\" variable\n",
        "print(data)                       #print the \"data\" variable contents\n",
        "myfile.close()                    #close LAS file"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZChZCkWxFBq6",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Congratulations**!\n",
        "now you can read LAS without any special package like LASIO or stuff and brag about it on LinkedIn.\n",
        "\n",
        "If that's not enough for you,\n",
        "to actually parse information and curve data in a LAS file, let's go to the next step.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nYwyfXSuFeM",
        "colab_type": "text"
      },
      "source": [
        "As you can see, LAS files are in fact a textual file (the other type is binary file).\n",
        "It means that we are facing a textual type of data. \n",
        "*(yeah, duh!)*\n",
        "\n",
        "To get value what we wants, we have to find pattern of the data inside those bunch of text.\n",
        "\n",
        "Some textual files got pattern like \"comma\" or \"tab\" as a delimiter (like \".csv\" type of files) that we can easily parse info from those text to an array. Luckily, LAS file is just like them and even with its own standarts.\n",
        "\n",
        "You can read it more on : https://www.cwls.org/wp-content/uploads/2014/09/LAS_3_File_Structure.pdf \n",
        "\n",
        "While the others type of textual files are just awful with their .asc, .txt, unfamilliar extension or even without extension name which got no pattern or how to extract those info.\n",
        "\n",
        "---\n",
        "\n",
        "**My data got no extention name, How do I know if I'm having a textual data not a binary one? and How to read those? There is a lot type of well data on my Company!**\n",
        "\n",
        "> Drag it into a notepad see if it got something that you can read, if yes then it's a textual files. (*in most cases, yes*)\n",
        "\n",
        "> I hope after learning these you use the same method to analyze and fetch that data Or maybe I can do some tutorial handling special case like that.\n",
        "\n",
        "> Yeah I feel you, there are just a lot of format just for one type of data. Probably what happened was like this (*credit to original XKCD and and some random meme*)\n",
        "\n",
        "![Geosciences Data](https://imgs.xkcd.com/comics/standards.png)\n",
        "\n",
        "or this..\n",
        "\n",
        "![yeah](https://i.ibb.co/j51XmN3/standards.png)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gSY61wdB5CD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"data type :\", type(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn-a-DVTuZMF",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Now, getting back to our data, let's see what it really does in our codes.\n",
        "\n",
        "As you can see, the code return each line of LAS content until the end of file.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wcyy7CWzjw3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filename=\"4771-36-SESE.las\"             #put file's name to filename variable\n",
        "myfile = open(filename)                 #open LAS file\n",
        "for num, line in enumerate(myfile, 1):  #loop for each line in myfile, put line number in \"num\" & line content in \"line\"\n",
        "  print(num)                            #display current no. of line\n",
        "  print(line)                           #display current content of line\n",
        "myfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfB6aRb140KZ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Have you read the standart yet? or at least take a look at your LAS data?\n",
        "\n",
        "Now you know that the data that we actually want, the well log curve value, is actually started after this flag :\n",
        "\n",
        "`~A` or `~Ascii`\n",
        "\n",
        "As the standart stated, those are the Column Data sections that corresponds to Log curve data.\n",
        "\n",
        "Now let's try to add those `~A` to our code and make a conditional where if those stuff found on certain lines, then we keep the line number.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKWanaHIj3SX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag_data=\"~A\"                                      #data flag\n",
        "drow_start=0                                        #a counter start from zero to find at which line it will found the data\n",
        "myfile = open(filename)\n",
        "for num, line in enumerate(myfile, 1):\n",
        "  if flag_data in line:                             #if this line contains ~A flag,\n",
        "    drow_start=num                                  #then put it to drow_start\n",
        "    print(\"log value found at line : \",drow_start)\n",
        "myfile.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFFzo4ma6WkI",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "After finding out after which line the data placed on those text, then we read the data using command bellow.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJm7QcPLnRbj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_data=np.loadtxt(filename,skiprows=drow_start) #reading the files, skip to lines where the log value located\n",
        "log_data.shape                                    #check the size array of data. As you can see it's a 2D Array."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNCNmRlD61ZR",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Nice. Anyway, see that `np.loadtxt` ? It's one of numpy feature to load a textual files and put it to a numpy array. In conclusion, those well log curves are now saved on `log_data` variable\n",
        "\n",
        "If you don't believe it then let's try to plot it.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1mIyKO_Qg5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt                  #package for plotting\n",
        "num_plot=log_data.shape[1]-1\n",
        "fig, ax = plt.subplots(1, num_plot, sharey=True) #making subplot figure of 1 x 4 (4 plot horizontally)\n",
        "                                                 #where they are sharing the same y-axis (depth)\n",
        "fig.suptitle('Test Plot')\n",
        "for i in range(0,num_plot):                      #looping for number of plot available,\n",
        "  ax[i].plot(log_data[:,i+1],log_data[:,0])      #plot log_data in subplot(ax) number i \n",
        "ax[-1].invert_yaxis()                            #inverting y-axis of the last plot, because sharey=True, all of their y-axis is affected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yADZvB687MWq",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**NOICE!**\n",
        "\n",
        "**But wait, Where's the axis? What curve is that? Is that in meters? That's really deep!**\n",
        "\n",
        "There some stuffs that we haven't parsed yet! \n",
        "For example:\n",
        "1. Curve information : Where curve type and their units located\n",
        "2. Well Parameter : Where measurements info like Starting Depth, End Depth, Step, and Null Data located.\n",
        "\n",
        "\n",
        "Now let's start finding their curve type and unit first.\n",
        "The steps are like this :\n",
        "1. Declare the flag `~C` which means its a Curve information section.\n",
        "2. Find at which line it starts and ends\n",
        "3. Make an exclusion that `#` need to be skipped\n",
        "4. Make an empty array for curve type and curve unit (empty because we don't know how many it actually is)\n",
        "5. Find a line which got curve type and unit, fetch each type and unit (I had to split it by \".\" because that's the pattern to write those on LAS apparently)\n",
        "6. Add (append) the data found on each array.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6pbt3K-0Oqk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flag_curveinfo=\"~C\" #flag where Curve Information located\n",
        "flag_other=\"~\"      #flag after curve info flag where we try to find which line where the Curve Information ended\n",
        "skipchar=\"#\"\n",
        "crow_start=0        #a counter start from zero to find at which line it will found the curve parameter starts\n",
        "crow_end=999999     #a counter start from 999999 to find at which line it will found the curve parameter ends\n",
        "type_curve=[]       #an empty array where we will put the Curve type\n",
        "unit_curve=[]       #an empty array where we will put the Curve type\n",
        "\n",
        "myfile = open(filename)\n",
        "for num, line in enumerate(myfile, 1):\n",
        "  if flag_curveinfo in line:\n",
        "    crow_start=num \n",
        "    print(\"curve info found at line : \",crow_start)\n",
        "  if (flag_other in line) and (num > crow_start) and (crow_start is not 0) and (crow_end == 999999):\n",
        "    crow_end=num \n",
        "    print(\"curve info ends at line : \",crow_end)\n",
        "  if not(skipchar in line) and (crow_start<num<crow_end) and (crow_start > 0) :\n",
        "    temp=line.split('.')\n",
        "    tempc=temp[0]\n",
        "    if (' ' in tempc):\n",
        "      tempc=tempc.split()\n",
        "      type_curve.append(tempc[0])\n",
        "    else:\n",
        "      type_curve.append(tempc)\n",
        "    tempu=temp[1]      \n",
        "    if (' ' in tempu):\n",
        "      tempu=tempu.split()\n",
        "      unit_curve.append(tempu[0])\n",
        "    else:\n",
        "      unit_curve.append(tempu)\n",
        "\n",
        "myfile.close()\n",
        "print(\"List Curve :\", type_curve)\n",
        "print(\"Unit Curve :\", unit_curve)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEPa94e9-rSM",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Beautiful!**\n",
        "\n",
        "We got the curve info. Each in `type_curve` and `unit_curve`\n",
        "\n",
        "Now let's try to get the well parameter info.\n",
        "The steps are just like above but with little edit :\n",
        "1. Declare the flag `~W` which means its a Well Parameter Info section.\n",
        "2. Find at which line it starts and ends\n",
        "3. Make an exclusion that `#` need to be skipped\n",
        "4. Make an empty important well parameter info variables. \n",
        "5. Find a line which got text: STRT, STOP, STEP, & NULL, then fetch each type and unit (I had use regex to find any number type of data. As you can see, people just write whatever they want on those parameter info without any certain rules. For example see this LAS on https://www.spec2000.net/08-lasfiles.htm#b4)\n",
        "6. Update those values on each variables.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVhWeDua87BN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re           #it's for getting certain format of data on a string/text. It's like regex.\n",
        "flag_wellinfo=\"~W\"  #flag where Well Information located\n",
        "flag_other=\"~\"      #flag after Well info flag where we try to find which line where the Well Information ended\n",
        "skipchar=\"#\"\n",
        "wrow_start=0        #a counter start from zero to find at which line it will found the Well info starts\n",
        "wrow_end=999999     #a counter start from 999999 to find at which line it will found the Well info ends\n",
        "strt_value=0\n",
        "stop_value=0\n",
        "step_value=0\n",
        "null_value=0\n",
        "\n",
        "myfile = open(filename)\n",
        "for num, line in enumerate(myfile, 1):\n",
        "  if flag_wellinfo in line:\n",
        "    wrow_start=num \n",
        "    print(\"well info found at line : \",wrow_start)\n",
        "  if (flag_other in line) and (num > wrow_start) and (wrow_start is not 0) and (wrow_end == 999999):\n",
        "    wrow_end=num \n",
        "    print(\"well info ends at line : \",wrow_end)\n",
        "  if not(skipchar in line) and (wrow_start<num<wrow_end)and ('STRT' in line) and (wrow_start > 0):\n",
        "    strt_value=float(re.findall(\"[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?\", line)[0]) #need regex to get null value number\n",
        "  if not(skipchar in line) and (wrow_start<num<wrow_end)and ('STOP' in line) and (wrow_start > 0):\n",
        "    stop_value=float(re.findall(\"[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?\", line)[0]) #need regex to get null value number\n",
        "  if not(skipchar in line) and (wrow_start<num<wrow_end)and ('STEP' in line) and (wrow_start > 0):\n",
        "    step_value=float(re.findall(\"[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?\", line)[0]) #need regex to get null value number\n",
        "  if not(skipchar in line) and (wrow_start<num<wrow_end)and ('NULL' in line) and (wrow_start > 0):\n",
        "    null_value=float(re.findall(\"[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?\", line)[0]) #need regex to get null value number\n",
        "\n",
        "myfile.close()\n",
        "print(\"Start Depth :\", strt_value)\n",
        "print(\"Stop Depth  :\", stop_value)\n",
        "print(\"Step        :\", step_value)\n",
        "print(\"Null Value  :\", null_value)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwIyQND4BN1J",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Awesome. Now we got most stuff that we need.\n",
        "\n",
        "Now let's change the Null value on the array. As you know, -999.25 or stuff from LAS that declare that as Null value is in fact not recognized as null value in python.\n",
        "\n",
        "We can find and change each value on `log_data` array using below lines of code.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGyFc-QB2shn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_data==null_value # getting logical value where value is equal to null_value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2GLt03kMMRE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_data[log_data==null_value]=np.nan #change it to a nan"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpZmsCzGmU2o",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Okay then, let's plot all of our fetched data here.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI_p5_ocQMmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt                             #package for plotting\n",
        "num_plot=log_data.shape[1]-1                                #number of plot, it's horizontal array size minus 1 because DEPTH is actualy our axis\n",
        "fig, ax = plt.subplots(1, num_plot, sharey=True)            #making subplot figure of 1 x 4 (4 plot horizontally)\n",
        "                                                            #where they are sharing the same y-axis (depth)\n",
        "fig.suptitle('LAS PLOT')                                    #the title\n",
        "for i in range(0,num_plot):                                 #looping for number of plot available,\n",
        "  ax[i].plot(log_data[:,i+1],log_data[:,0])                 #plot log_data in subplot(ax) number i \n",
        "  ax[i].set_title(type_curve[i+1]+\" (\"+unit_curve[i+1]+\")\") #set the x-label of subplot(ax) number i using our array that we got from above \n",
        "ax[-1].invert_yaxis()                                       #inverting y-axis of the last plot, because sharey=True, all of their y-axis is affected\n",
        "ax[0].set(ylabel=type_curve[0]+\" (\"+unit_curve[0]+\")\")      #set the x-label of first subplot(ax)(i=0) with depth label and it's unit."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmtVf7qknb04",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Wait, something's feels off. \n",
        "\n",
        "Oh right, let's change the resistivity `x-axis` a log scale (post 90s log  usually use this).\n",
        "\n",
        "And also reverse the `x-axis` of sonic log.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8Nvpd6QTfpu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "num_plot=log_data.shape[1]-1\n",
        "fig, ax = plt.subplots(1, num_plot, sharey=True)            \n",
        "                                                            \n",
        "fig.suptitle('LAS PLOT')\n",
        "for i in range(0,num_plot):\n",
        "  ax[i].plot(log_data[:,i+1],log_data[:,0])\n",
        "  ax[i].set_title(type_curve[i+1]+\" (\"+unit_curve[i+1]+\")\")\n",
        "ax[-1].invert_yaxis()                                       \n",
        "ax[0].set(ylabel=type_curve[0]+\" (\"+unit_curve[0]+\")\")\n",
        "ax[1].set_xscale('log')                                     #setting x-scale because that's what RES usually looks like (well, newer kind of RES at least)\n",
        "ax[0].invert_xaxis()                                        #inverting x-axis because that's what DT usually looks like"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atKpb4V0pMw9",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "You can also convert it to a `pandas` dataframe by using lines of code below. (Optional)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bq-bkuGpNEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd                                     \n",
        "\n",
        "log_df = pd.DataFrame(data=log_data,columns=type_curve) #make panda dataframe from log_data array with type_curve\n",
        "                                                        #as column index\n",
        "log_df.head(5)                                          #show first 5 of curve data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tixQGZvJrNd_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_df.describe() #show basic statistical description of our data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFhLNSyaX1qZ",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "# **WRAPPING UP!** Let's Put it on a Function\n",
        "\n",
        "Alright, we understand how to parse all those important data and info, now we can summ it up into a **function**, which you can use it by calling a simple command everytime you need to fetch one or multiple LAS file.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWewY07tY_Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_las(filename):         #make a function called read_las which pass a filename of the las we want to parse\n",
        "  import numpy as np \n",
        "  import re\n",
        "  flag_data=\"~A\"                                      \n",
        "  flag_curveinfo=\"~C\" \n",
        "  flag_other=\"~\"      \n",
        "  skipchar=\"#\"\n",
        "  flag_wellinfo=\"~W\"       \n",
        "  drow_start=0\n",
        "  wrow_start=0        \n",
        "  wrow_end=999999\n",
        "  crow_start=0        \n",
        "  crow_end=999999       \n",
        "  type_curve=[]       \n",
        "  unit_curve=[]\n",
        "\n",
        "  myfile = open(filename)\n",
        "  for num, line in enumerate(myfile, 1):\n",
        "    if flag_data in line:                             \n",
        "      drow_start=num\n",
        "    if flag_curveinfo in line:\n",
        "      crow_start=num \n",
        "    if (flag_other in line) and (num > crow_start) and (crow_start is not 0) and (crow_end == 999999):\n",
        "      crow_end=num \n",
        "    if not(skipchar in line) and (crow_start<num<crow_end) and (crow_start > 0) :\n",
        "      temp=line.split('.')\n",
        "      tempc=temp[0]\n",
        "      if (' ' in tempc):\n",
        "        tempc=tempc.split()\n",
        "        type_curve.append(tempc[0])\n",
        "      else:\n",
        "        type_curve.append(tempc)\n",
        "      tempu=temp[1]      \n",
        "      if (' ' in tempu):\n",
        "        tempu=tempu.split()\n",
        "        unit_curve.append(tempu[0])\n",
        "      else:\n",
        "        unit_curve.append(tempu)\n",
        "    if flag_wellinfo in line:\n",
        "      wrow_start=num \n",
        "    if (flag_other in line) and (num > wrow_start) and (wrow_start is not 0) and (wrow_end == 999999):\n",
        "      wrow_end=num \n",
        "    if not(skipchar in line) and (wrow_start<num<wrow_end)and ('STRT' in line) and (wrow_start > 0):\n",
        "      strt_value=float(re.findall(\"[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?\", line)[0]) #need regex to get null value number\n",
        "    if not(skipchar in line) and (wrow_start<num<wrow_end)and ('STOP' in line) and (wrow_start > 0):\n",
        "      stop_value=float(re.findall(\"[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?\", line)[0]) #need regex to get null value number\n",
        "    if not(skipchar in line) and (wrow_start<num<wrow_end)and ('STEP' in line) and (wrow_start > 0):\n",
        "      step_value=float(re.findall(\"[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?\", line)[0]) #need regex to get null value number\n",
        "    if not(skipchar in line) and (wrow_start<num<wrow_end)and ('NULL' in line) and (wrow_start > 0):\n",
        "      null_value=float(re.findall(\"[-+]?(?:(?:\\d*\\.\\d+)|(?:\\d+\\.?))(?:[Ee][+-]?\\d+)?\", line)[0]) #need regex to get null value number\n",
        "  log_data=np.loadtxt(filename,skiprows=drow_start)\n",
        "  log_data[log_data==null_value]=np.nan\n",
        "  myfile.close()\n",
        "  return log_data,strt_value,stop_value,step_value,null_value,type_curve,unit_curve"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGrTOwvJZ1pp",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "**Wait that's much more shorter than before.**\n",
        "\n",
        "\n",
        "> Well yes, that's because I slice it up to explain it easier step-by-step. It's actually just that simple to parse a LAS Files.\n",
        "\n",
        "\n",
        "Now let's test our new function.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz2biJGLaVvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "log_data,strt_value,stop_value,step_value,null_value,type_curve,unit_curve = read_las('4771-36-SESE.las')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XvR9LpG5Z9L_",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Boom! those file you just read now on `log_data`,`strt_value`,`stop_value`,`step_value`,`null_value`,`type_curve`, and `unit_curve`.\n",
        "\n",
        "Let's try to plot those variables.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74GfGVrpaxEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "num_plot=log_data.shape[1]-1\n",
        "fig, ax = plt.subplots(1, num_plot, sharey=True)            \n",
        "                                                            \n",
        "fig.suptitle('LAS PLOT')\n",
        "for i in range(0,num_plot):\n",
        "  ax[i].plot(log_data[:,i+1],log_data[:,0])\n",
        "  ax[i].set_title(type_curve[i+1]+\" (\"+unit_curve[i+1]+\")\")\n",
        "ax[-1].invert_yaxis()                                       \n",
        "ax[0].set(ylabel=type_curve[0]+\" (\"+unit_curve[0]+\")\")\n",
        "ax[1].set_xscale('log')                                     \n",
        "ax[0].invert_xaxis()                                        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AshCWegaMVI",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "Alright, that's it!\n",
        "You get your data and important info to analyze your data further.\n",
        "I hope you can learn something from this tutorial.\n",
        "\n",
        "\n",
        "In the next session, I'll try to share how to read some seismic data from scratch.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoWzrsN9M5ep",
        "colab_type": "text"
      },
      "source": [
        "# **EXERCISES**\n",
        "\n",
        "1. Can you check whether the data got the same step value as it is informed on the header (well information)?\n",
        "2. Can you check whether the data is complete as it is informed on the header (well information)? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_NRVxaDM_oh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(code here)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}